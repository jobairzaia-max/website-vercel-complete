import requests
import re
import time
import json
import os
from bs4 import BeautifulSoup
from datetime import datetime, timedelta
from urllib.parse import urljoin

# ==============================================
# å›½å®¶çº§+çœçº§æ”¿ç­–æºï¼ˆå«é«˜æ–°/ä¸“ç²¾ç‰¹æ–°è®¤è¯ï¼‰
# ==============================================
TARGET_DEPARTMENTS = {
    # å›½å®¶çº§éƒ¨é—¨ï¼ˆæ–°å¢ï¼‰
    "å·¥ä¿¡éƒ¨": {
        "base_url": "https://www.miit.gov.cn/",
        "policy_urls": ["https://www.miit.gov.cn/ztzl/zhuanjingtexin/"],  # ä¸“ç²¾ç‰¹æ–°è®¤è¯
        "keywords": ["ä¸“ç²¾ç‰¹æ–°", "å°å·¨äººä¼ä¸š", "ä¸“é¡¹åŸ¹è‚²"]
    },
    "ç§‘æŠ€éƒ¨": {
        "base_url": "http://www.most.gov.cn/",
        "policy_urls": ["http://www.most.gov.cn/ztzl/gxqyrd/"],  # é«˜æ–°ä¼ä¸šè®¤å®š
        "keywords": ["é«˜æ–°æŠ€æœ¯ä¼ä¸š", "ç ”å‘è´¹ç”¨", "ç§‘æŠ€å‹ä¸­å°ä¼ä¸š"]
    },
    # çœçº§éƒ¨é—¨ï¼ˆä¿ç•™ï¼‰
    "ç¦å»ºçœæ”¿åºœ": {
        "base_url": "https://www.fujian.gov.cn/",
        "policy_urls": ["https://www.fujian.gov.cn/zwgk/ztzl/hqzc/"],
        "keywords": ["æƒ ä¼", "ä¸“é¡¹èµ„é‡‘", "æ‰¶æŒ"]
    }
}

# å†å²æ”¿ç­–å½’æ¡£è·¯å¾„
HISTORICAL_POLICIES_PATH = os.path.abspath(os.path.join(
    os.getcwd(), "../public/historical_policies.json"
))
LATEST_POLICIES_PATH = os.path.abspath(os.path.join(
    os.getcwd(), "../public/policy_data.json"
))

# ==============================================
# æ ¸å¿ƒåŠŸèƒ½ï¼šæŠ“å–+å½’æ¡£+èåˆå†å²æ”¿ç­–
# ==============================================
def åˆè§„è¯·æ±‚(url):
    headers = {"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) Chrome/120.0.0.0 Safari/537.36"}
    try:
        response = requests.get(url, headers=headers, timeout=15)
        return response if response.status_code == 200 else None
    except:
        return None

def load_historical_policies():
    """åŠ è½½å†å²æ”¿ç­–å¤‡ä»½"""
    if os.path.exists(HISTORICAL_POLICIES_PATH):
        with open(HISTORICAL_POLICIES_PATH, "r", encoding="utf-8") as f:
            return json.load(f).get("policies", [])
    return []

def save_historical_policies(new_policies):
    """åˆå¹¶æ–°æ”¿ç­–åˆ°å†å²å½’æ¡£ï¼ˆå»é‡ï¼‰"""
    historical = load_historical_policies()
    combined = historical + new_policies
    # æŒ‰æ ‡é¢˜å»é‡ï¼Œä¿ç•™æœ€æ–°ç‰ˆæœ¬
    unique_combined = {p["title"][:50]: p for p in combined}.values()
    # åªä¿ç•™è¿‘3ä¸ªæœˆæ”¿ç­–
    three_months_ago = (datetime.now() - timedelta(days=90)).strftime("%Y-%m-%d")
    filtered = [p for p in unique_combined if p["date"] >= three_months_ago]
    with open(HISTORICAL_POLICIES_PATH, "w", encoding="utf-8") as f:
        json.dump({"update_time": datetime.now().strftime("%Y-%m-%d"), "policies": filtered}, f, ensure_ascii=False, indent=2)

def crawl_daily_policies():
    # 1. æŠ“å–ä»Šæ—¥æ”¿ç­–
    today_policies = []
    print("ğŸ¯ å¼€å§‹æŠ“å–å›½å®¶çº§+çœçº§æ”¿ç­–ï¼ˆå«é«˜æ–°/ä¸“ç²¾ç‰¹æ–°è®¤è¯ï¼‰...")
    for dept_name, config in TARGET_DEPARTMENTS.items():
        print(f"\nğŸ” æ­£åœ¨è®¿é—®ï¼š{dept_name}")
        for url in config["policy_urls"]:
            response = åˆè§„è¯·æ±‚(url)
            if not response:
                continue
            soup = BeautifulSoup(response.text, "html.parser")
            for item in soup.find_all("li")[:30]:
                title_tag = item.find("a", href=True)
                if not title_tag:
                    continue
                title = title_tag.text.strip()
                if len(title) < 10:
                    continue
                # å…³é”®è¯åŒ¹é…ï¼ˆå¦‚"é«˜æ–°ä¼ä¸š""ä¸“ç²¾ç‰¹æ–°"ï¼‰
                if any(kw in title for kw in config["keywords"]):
                    date_str = re.search(r"\d{4}-\d{2}-\d{2}", item.text) or re.search(r"\d{4}å¹´\d{2}æœˆ\d{2}æ—¥", item.text)
                    if date_str:
                        normalized_date = date_str.group().replace("å¹´", "-").replace("æœˆ", "-").replace("æ—¥", "")
                        today_policies.append({
                            "title": title,
                            "date": normalized_date,
                            "url": urljoin(config["base_url"], title_tag["href"]),
                            "department": dept_name,
                            "type": "å›½å®¶çº§" if dept_name in ["å·¥ä¿¡éƒ¨", "ç§‘æŠ€éƒ¨", "è´¢æ”¿éƒ¨"] else "çœçº§"
                        })
                        print(f"âœ… å‘ç°æ”¿ç­–ï¼š{title}")

    # 2. ä¿å­˜ä»Šæ—¥æ”¿ç­–+æ›´æ–°å†å²å½’æ¡£
    with open(LATEST_POLICIES_PATH, "w", encoding="utf-8") as f:
        json.dump({"total": len(today_policies), "policies": today_policies}, f, ensure_ascii=False, indent=2)
    save_historical_policies(today_policies)  # åˆå¹¶åˆ°å†å²å½’æ¡£
    print(f"\nğŸ‰ æŠ“å–å®Œæˆï¼ä»Šæ—¥{len(today_policies)}æ¡ï¼Œå†å²å½’æ¡£{len(load_historical_policies())}æ¡")
    return today_policies

if __name__ == "__main__":
    crawl_daily_policies()